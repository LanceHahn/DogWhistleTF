According to a recent New York Times investigation, despite Elon Musk’s promise to remove child porn from Twitter, calling it “priority #1,” the company has actually fired staff dealing with the issue, stopped paying for important abuse material detection software, and the platform still hosts countless images and videos of child abuse material, with its algorithm even suggesting it to users.A recent report from the New York Times states that Twitter has been battling child sexual abuse content since Elon Musk stated in a tweet in November 2022 that “removing child exploitation is priority #1” Twitter’s head of safety, Ella Irwin, claims that she is quickly resolving the problem, which was widespread on the platform under its previous owners. Child sexual abuse material (CSAM), commonly known as child pornography, is still present on the platform, according to a recent investigation by the New York Times, and it includes widely disseminated content that authorities believe is simple to identify and remove.Hannibal Hanschke-Pool/Getty Images/BBN Edit)The analysis reportedly reveals that Twitter has failed to stop the spread of abusive images previously identified by authorities since Musk took over in late October. Twitter has largely eliminated or lost staff with experience dealing with the issue and the company has also stopped paying for important detection software for its operations. According to transcripts from an anti-abuse organization that surveils viewers of child pornography, people have been talking about how Twitter is still a platform where they can easily find the material without being discovered. Under Musk, Twitter has continued to struggle with images depicting child abuse. The Times established a personal Twitter account and developed an automated computer program to search the platform for the content without displaying the actual images, which are illegal to view, in order to evaluate the company’s claims of progress. The content was easy to find, and Twitter’s recommendation algorithm — a function that suggests accounts to follow based on user activity — actually assisted in its promotion.One of the suggested accounts had a boy without a shirt in his profile picture. According to the Canadian Center for Child Protection, which assisted The Times in finding exploitative content on the platform by comparing it to a database of previously recognized imagery, the child in the picture was a well-known victim of sexual abuse. The same user followed several dubious accounts, one of which had “liked” a video of a boy molesting another boy. On January 19, after more than a month on Twitter, the video had amassed over 122,000 views, nearly 300 retweets, and more than 2,600 likes. After the Canadian center reported the video to the company, Twitter removed it.Recent reports claim that Twitter’s relationship with the National Center for Missing and Exploited Children has deteriorated. The “high level of turnover” at Twitter and the firm’s dedication to finding and removing child sexual abuse material from its platform bothered John Shehan, an executive at the center. This has led some people to wonder if Twitter can be trusted and relied upon to protect young users on its platform.The relationship between the center and Twitter is reciprocal; the site alerts the center to any illegal content, and the center, in turn, requests that the site take down the offending images and accounts. Twitter has been slower to respond to the center’s notifications of sexual abuse content since it became Elon Musk’s company, according to the Times. Despite the center sending fewer alerts than in the previous year, the company’s response time before Musk took over was more than twice as fast, according to data from the center.Read more at the New York Times here.Lucas Nolan is a reporter for Breitbart News covering issues of free speech and online censorship. Follow him on Twitter @LucasNolan